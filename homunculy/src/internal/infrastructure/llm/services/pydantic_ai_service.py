"""PydanticAI LLM Service Implementation."""

import os
from typing import Any, Dict, Optional

from pydantic_ai import Agent as PydanticAgent, ModelSettings
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.openai import OpenAIProvider

from internal.domain.entities import AgentConfiguration, AgentResponse, AgentStatus
from internal.domain.services import LLMService


class PydanticAILLMService(LLMService):
    """PydanticAI-based LLM service implementation."""

    def __init__(self, api_key: Optional[str] = None) -> None:
        self.api_key = api_key or os.getenv("LLM_OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError("OpenAI API key not provided. Set LLM_OPENAI_API_KEY environment variable.")

    async def chat(
        self,
        configuration: AgentConfiguration,
        message: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> AgentResponse:
        try:
            personality = configuration.personality
            personality_context = (
                f"You are {personality.name}. {personality.description}. "
                f"Your current mood is {personality.mood}."
            )

            system_prompt = (configuration.system_prompt or "").strip()
            full_system_prompt = f"{system_prompt}\n\n{personality_context}".strip()

            provider = OpenAIProvider(api_key=self.api_key)
            model = OpenAIChatModel(configuration.model_name, provider=provider)

            agent = PydanticAgent(
                model,
                system_prompt=full_system_prompt,
                model_settings=ModelSettings(
                    temperature=configuration.temperature,
                    max_tokens=configuration.max_tokens,
                ),
            )

            result = await agent.run(message)
            response_text = result.output

            return AgentResponse(
                message=response_text,
                confidence=0.95,
                reasoning=f"Generated by {configuration.model_name} via PydanticAI",
                metadata={
                    "model": configuration.model_name,
                    "temperature": configuration.temperature,
                    "max_tokens": configuration.max_tokens,
                },
                status=AgentStatus.COMPLETED,
            )

        except Exception as exc:
            return AgentResponse(
                message=f"Error: {exc}",
                confidence=0.0,
                reasoning="Failed to communicate with LLM via PydanticAI",
                metadata={"error": str(exc)},
                status=AgentStatus.ERROR,
            )
