# RAG Service

ðŸ” **Retrieval-Augmented Generation Service** - Document ingestion pipeline and semantic search with Pinecone.

## Overview

| Aspect | Details |
|--------|---------|
| **Language** | Python 3.12+ |
| **Framework** | FastAPI |
| **Vector DB** | Pinecone (Local for dev, Cloud for prod) |
| **Embeddings** | OpenAI text-embedding-3-small |
| **Architecture** | Clean Architecture |

## Key Features

- ðŸ“„ **Document Ingestion** - Upload PDF, Markdown, Text files
- âœ‚ï¸ **Smart Chunking** - Configurable text splitting with overlap
- ðŸ§  **Embedding Generation** - OpenAI embeddings for semantic search
- ðŸ” **Semantic Search** - Query documents with natural language
- ðŸ³ **Pinecone Local** - Docker-based development environment

## Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document â”‚â”€â”€â”€â–¶â”‚ Chunking â”‚â”€â”€â”€â–¶â”‚ Embeddingâ”‚â”€â”€â”€â–¶â”‚ Pinecone â”‚
â”‚  Upload  â”‚    â”‚ (512 tok)â”‚    â”‚ (OpenAI) â”‚    â”‚  Upsert  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
rag-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # FastAPI entry point
â”‚   â”œâ”€â”€ settings/                  # Configuration
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ pinecone.py
â”‚   â””â”€â”€ internal/
â”‚       â”œâ”€â”€ domain/                # Entities & interfaces
â”‚       â”‚   â”œâ”€â”€ entities/
â”‚       â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ usecases/              # Business logic
â”‚       â”‚   â”œâ”€â”€ ingest.py
â”‚       â”‚   â””â”€â”€ retrieve.py
â”‚       â”œâ”€â”€ adapters/              # HTTP handlers
â”‚       â”‚   â””â”€â”€ http/
â”‚       â””â”€â”€ infrastructure/        # External services
â”‚           â”œâ”€â”€ pinecone/
â”‚           â””â”€â”€ embeddings/
â”œâ”€â”€ tests/
â”œâ”€â”€ Dockerfile
â””â”€â”€ pyproject.toml
```

## Quick Start

### 1. Start Pinecone Local (Docker)

```bash
docker run -d \
  --name pinecone-local \
  -e PORT=5081 \
  -e INDEX_TYPE=serverless \
  -e DIMENSION=1536 \
  -e METRIC=cosine \
  -p 5081:5081 \
  ghcr.io/pinecone-io/pinecone-index:latest
```

### 2. Install & Run

```bash
cd rag-service
poetry install
poetry run uvicorn src.main:app --reload --port 8001
```

### 3. Environment Variables

```env
# Pinecone
PINECONE_ENVIRONMENT=local          # 'local' or 'cloud'
PINECONE_API_KEY=pclocal            # Any value for local
PINECONE_HOST=localhost:5081        # Pinecone Local host
PINECONE_INDEX_NAME=homunculy-rag

# OpenAI (for embeddings)
OPENAI_API_KEY=sk-...

# RAG Settings
RAG_CHUNK_SIZE=512
RAG_CHUNK_OVERLAP=50
RAG_TOP_K=5
```

## API Endpoints

### Ingest Documents

```bash
# Upload a document
curl -X POST http://localhost:8001/api/v1/ingest \
  -F "file=@document.pdf" \
  -F "metadata={\"source\": \"manual\", \"category\": \"docs\"}"

# Ingest text directly
curl -X POST http://localhost:8001/api/v1/ingest/text \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Your document content here...",
    "metadata": {"source": "api", "title": "My Doc"}
  }'
```

### Query Documents

```bash
curl -X POST http://localhost:8001/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the architecture of the system?",
    "top_k": 5,
    "filter": {"category": "docs"}
  }'
```

### Response Format

```json
{
  "results": [
    {
      "id": "doc_chunk_001",
      "text": "The system uses Clean Architecture...",
      "score": 0.92,
      "metadata": {
        "source": "architecture.md",
        "chunk_index": 0
      }
    }
  ],
  "query": "What is the architecture?",
  "total_results": 5
}
```

## Integration with Homunculy Agent

The RAG service exposes a simple API that the Homunculy agent can call:

```python
# In homunculy agent tool
async def retrieve_context(query: str) -> list[str]:
    response = await httpx.post(
        "http://rag-service:8001/api/v1/query",
        json={"query": query, "top_k": 5}
    )
    return [r["text"] for r in response.json()["results"]]
```

## Via Aspire (Recommended)

Run with the full stack:

```bash
cd ../aspire
dotnet run --project Homunculy.AppHost
```

## Testing

```bash
# Run tests
poetry run pytest

# With coverage
poetry run pytest --cov=src
```
